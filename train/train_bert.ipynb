{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46079748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace6ab7afe21446f906960e665a93018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"text\", data_files={\"train\": \"dataset/standard_hex.txt\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43589462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"dataset/test_tokenizer_folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e456eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertForMaskedLM\n",
    "\n",
    "config = BertConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=256,\n",
    "    num_hidden_layers=6,\n",
    "    num_attention_heads=4,\n",
    "    max_position_embeddings=32,\n",
    ")\n",
    "model = BertForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e04f110",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hs/p_t7843d0wdc8q0z06yq2xh80000gn/T/ipykernel_46659/4166094276.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    eval_steps=500,\n",
    "    save_steps=1000,\n",
    "    per_device_train_batch_size=64,\n",
    "    num_train_epochs=10,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    data_collator=collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0b42466",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch, sklearn.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/mythezone/fin-bert/4b305fd12f7248d6953fccab805e99c6\n",
      "\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch, sklearn.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Not all initial data has been logged for experiment 4b305fd12f7248d6953fccab805e99c6, call Experiment.end() to ensure that all data to have been logged\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : crowded_prawn_3097\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/mythezone/fin-bert/4b305fd12f7248d6953fccab805e99c6\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hasNestedParams : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|accelerator_config|dispatch_batches             : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|accelerator_config|even_batches                 : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|accelerator_config|gradient_accumulation_kwargs : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|accelerator_config|non_blocking                 : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|accelerator_config|split_batches                : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|accelerator_config|use_seedable_sampler         : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|adafactor                                       : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|adam_beta1                                      : 0.9\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|adam_beta2                                      : 0.999\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|adam_epsilon                                    : 1e-08\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|auto_find_batch_size                            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|average_tokens_across_devices                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|batch_eval_metrics                              : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|bf16                                            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|bf16_full_eval                                  : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|data_seed                                       : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|dataloader_drop_last                            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|dataloader_num_workers                          : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|dataloader_persistent_workers                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|dataloader_pin_memory                           : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|dataloader_prefetch_factor                      : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|ddp_backend                                     : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|ddp_broadcast_buffers                           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|ddp_bucket_cap_mb                               : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|ddp_find_unused_parameters                      : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|ddp_timeout                                     : 1800\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|debug                                           : []\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|deepspeed                                       : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|disable_tqdm                                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|do_eval                                         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|do_predict                                      : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|do_train                                        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|eval_accumulation_steps                         : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|eval_delay                                      : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|eval_do_concat_batches                          : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|eval_on_start                                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|eval_steps                                      : 500\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|eval_strategy                                   : no\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|eval_use_gather_object                          : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|fp16                                            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|fp16_backend                                    : auto\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|fp16_full_eval                                  : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|fp16_opt_level                                  : O1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|fsdp                                            : []\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|fsdp_config|min_num_params                      : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|fsdp_config|xla                                 : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|fsdp_config|xla_fsdp_grad_ckpt                  : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|fsdp_config|xla_fsdp_v2                         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|fsdp_min_num_params                             : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|fsdp_transformer_layer_cls_to_wrap              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|full_determinism                                : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|gradient_accumulation_steps                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|gradient_checkpointing                          : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|gradient_checkpointing_kwargs                   : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|greater_is_better                               : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|group_by_length                                 : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|half_precision_backend                          : auto\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|hub_always_push                                 : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|hub_model_id                                    : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|hub_private_repo                                : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|hub_strategy                                    : every_save\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|hub_token                                       : <HUB_TOKEN>\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|ignore_data_skip                                : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|include_for_metrics                             : []\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|include_inputs_for_metrics                      : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|include_num_input_tokens_seen                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|include_tokens_per_second                       : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|jit_mode_eval                                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|label_names                                     : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|label_smoothing_factor                          : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|learning_rate                                   : 5e-05\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|length_column_name                              : length\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|load_best_model_at_end                          : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|local_rank                                      : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|log_level                                       : passive\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|log_level_replica                               : warning\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|log_on_each_node                                : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|logging_dir                                     : ./output/runs/May08_21-47-57_Mythezones-Mac-mini-M4.local\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|logging_first_step                              : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|logging_nan_inf_filter                          : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|logging_steps                                   : 500\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|logging_strategy                                : steps\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|lr_scheduler_kwargs                             : {}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|lr_scheduler_type                               : linear\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|max_grad_norm                                   : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|max_steps                                       : -1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|metric_for_best_model                           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|mp_parameters                                   : \n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|neftune_noise_alpha                             : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|no_cuda                                         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|num_train_epochs                                : 10\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|optim                                           : adamw_torch\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|optim_args                                      : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|optim_target_modules                            : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|output_dir                                      : ./output\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|overwrite_output_dir                            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|past_index                                      : -1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|per_device_eval_batch_size                      : 8\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|per_device_train_batch_size                     : 64\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|per_gpu_eval_batch_size                         : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|per_gpu_train_batch_size                        : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|prediction_loss_only                            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|push_to_hub                                     : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|push_to_hub_model_id                            : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|push_to_hub_organization                        : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|push_to_hub_token                               : <PUSH_TO_HUB_TOKEN>\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|ray_scope                                       : last\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|remove_unused_columns                           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|report_to                                       : ['comet_ml', 'mlflow', 'wandb']\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|restore_callback_states_from_checkpoint         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|resume_from_checkpoint                          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|run_name                                        : ./output\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|save_on_each_node                               : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|save_only_model                                 : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|save_safetensors                                : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|save_steps                                      : 1000\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|save_strategy                                   : steps\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|save_total_limit                                : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|seed                                            : 42\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|skip_memory_metrics                             : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|tf32                                            : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|torch_compile                                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|torch_compile_backend                           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|torch_compile_mode                              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|torch_empty_cache_steps                         : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|torchdynamo                                     : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|tp_size                                         : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|tpu_metrics_debug                               : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|tpu_num_cores                                   : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|use_cpu                                         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|use_ipex                                        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|use_legacy_prediction_loop                      : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|use_liger_kernel                                : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|use_mps_device                                  : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|warmup_ratio                                    : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|warmup_steps                                    : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|weight_decay                                    : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|_attn_implementation_autoset                  : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|_name_or_path                                 : \n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|add_cross_attention                           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|architectures                                 : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|attention_probs_dropout_prob                  : 0.1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|bad_words_ids                                 : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|begin_suppress_tokens                         : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|bos_token_id                                  : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|chunk_size_feed_forward                       : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|classifier_dropout                            : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|cross_attention_hidden_size                   : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|decoder_start_token_id                        : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|diversity_penalty                             : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|do_sample                                     : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|early_stopping                                : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|encoder_no_repeat_ngram_size                  : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|eos_token_id                                  : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|exponential_decay_length_penalty              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|finetuning_task                               : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|forced_bos_token_id                           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|forced_eos_token_id                           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|hidden_act                                    : gelu\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|hidden_dropout_prob                           : 0.1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|hidden_size                                   : 256\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|id2label|0                                    : LABEL_0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|id2label|1                                    : LABEL_1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|initializer_range                             : 0.02\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|intermediate_size                             : 3072\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|is_decoder                                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|is_encoder_decoder                            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|label2id|LABEL_0                              : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|label2id|LABEL_1                              : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|layer_norm_eps                                : 1e-12\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|length_penalty                                : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|max_length                                    : 20\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|max_position_embeddings                       : 32\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|min_length                                    : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|model_type                                    : bert\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|no_repeat_ngram_size                          : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|num_attention_heads                           : 4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|num_beam_groups                               : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|num_beams                                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|num_hidden_layers                             : 6\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|num_return_sequences                          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|output_attentions                             : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|output_hidden_states                          : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|output_scores                                 : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|pad_token_id                                  : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|position_embedding_type                       : absolute\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|prefix                                        : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|problem_type                                  : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|pruned_heads                                  : {}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|remove_invalid_values                         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|repetition_penalty                            : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|return_dict                                   : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|return_dict_in_generate                       : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|sep_token_id                                  : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|suppress_tokens                               : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|task_specific_params                          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|temperature                                   : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|tf_legacy_loss                                : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|tie_encoder_decoder                           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|tie_word_embeddings                           : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|tokenizer_class                               : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|top_k                                         : 50\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|top_p                                         : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|torch_dtype                                   : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|torchscript                                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|transformers_version                          : 4.51.3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|type_vocab_size                               : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|typical_p                                     : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|use_bfloat16                                  : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|use_cache                                     : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|vocab_size                                    : 181852\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details      : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata             : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git-patch (uncompressed) : 1 (3.97 KB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model graph              : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code              : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch, sklearn.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/mythezone/fin-bert/060b38bcfdff422c8160c061ba90e294\n",
      "\n",
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "wandb: Currently logged in as: mythezone (mythezone-sustech) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.3s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/mythezone/project/python/fin_bert/wandb/run-20250508_214828-xduviv5e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mythezone-sustech/huggingface/runs/xduviv5e' target=\"_blank\">./output</a></strong> to <a href='https://wandb.ai/mythezone-sustech/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mythezone-sustech/huggingface' target=\"_blank\">https://wandb.ai/mythezone-sustech/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mythezone-sustech/huggingface/runs/xduviv5e' target=\"_blank\">https://wandb.ai/mythezone-sustech/huggingface/runs/xduviv5e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mythezone/miniconda3/envs/hf/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['text']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hf/lib/python3.13/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hf/lib/python3.13/site-packages/transformers/trainer.py:2514\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2512\u001b[39m update_step += \u001b[32m1\u001b[39m\n\u001b[32m   2513\u001b[39m num_batches = args.gradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step != (total_updates - \u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[32m-> \u001b[39m\u001b[32m2514\u001b[39m batch_samples, num_items_in_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2515\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[32m   2516\u001b[39m     step += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hf/lib/python3.13/site-packages/transformers/trainer.py:5243\u001b[39m, in \u001b[36mTrainer.get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches, device)\u001b[39m\n\u001b[32m   5241\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m   5242\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5243\u001b[39m         batch_samples.append(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   5244\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   5245\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hf/lib/python3.13/site-packages/accelerate/data_loader.py:566\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[32m    565\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m     current_batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    568\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hf/lib/python3.13/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hf/lib/python3.13/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hf/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hf/lib/python3.13/site-packages/transformers/data/data_collator.py:46\u001b[39m, in \u001b[36mDataCollatorMixin.__call__\u001b[39m\u001b[34m(self, features, return_tensors)\u001b[39m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tf_call(features)\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m return_tensors == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtorch_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m return_tensors == \u001b[33m\"\u001b[39m\u001b[33mnp\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.numpy_call(features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hf/lib/python3.13/site-packages/transformers/data/data_collator.py:1013\u001b[39m, in \u001b[36mDataCollatorForLanguageModeling.torch_call\u001b[39m\u001b[34m(self, examples)\u001b[39m\n\u001b[32m   1010\u001b[39m     \u001b[38;5;28mself\u001b[39m.create_rng()\n\u001b[32m   1012\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(examples[\u001b[32m0\u001b[39m], Mapping):\n\u001b[32m-> \u001b[39m\u001b[32m1013\u001b[39m     batch = \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1017\u001b[39m     batch = {\n\u001b[32m   1018\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m: _torch_collate_batch(examples, \u001b[38;5;28mself\u001b[39m.tokenizer, pad_to_multiple_of=\u001b[38;5;28mself\u001b[39m.pad_to_multiple_of)\n\u001b[32m   1019\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hf/lib/python3.13/site-packages/transformers/data/data_collator.py:67\u001b[39m, in \u001b[36mpad_without_fast_tokenizer_warning\u001b[39m\u001b[34m(tokenizer, *pad_args, **pad_kwargs)\u001b[39m\n\u001b[32m     64\u001b[39m tokenizer.deprecation_warnings[\u001b[33m\"\u001b[39m\u001b[33mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     padded = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[32m     70\u001b[39m     tokenizer.deprecation_warnings[\u001b[33m\"\u001b[39m\u001b[33mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[33m\"\u001b[39m] = warning_state\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hf/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:3324\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.pad\u001b[39m\u001b[34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001b[39m\n\u001b[32m   3322\u001b[39m \u001b[38;5;66;03m# The model's main input name, usually `input_ids`, has been passed for padding\u001b[39;00m\n\u001b[32m   3323\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_input_names[\u001b[32m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m encoded_inputs:\n\u001b[32m-> \u001b[39m\u001b[32m3324\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3325\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3326\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mthat includes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.model_input_names[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, but you provided \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(encoded_inputs.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   3327\u001b[39m     )\n\u001b[32m   3329\u001b[39m required_input = encoded_inputs[\u001b[38;5;28mself\u001b[39m.model_input_names[\u001b[32m0\u001b[39m]]\n\u001b[32m   3331\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m required_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(required_input, Sized) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(required_input) == \u001b[32m0\u001b[39m):\n",
      "\u001b[31mValueError\u001b[39m: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['text']"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
