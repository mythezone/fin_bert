{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c417a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train.f import corpus_to_txt, token_tuple_to_str_slash\n",
    "from train.train_tokenizer import *\n",
    "\n",
    "\n",
    "corpus_df = pd.read_pickle(\"dataset/pickled/standard.pkl\")\n",
    "corpus_to_txt(corpus_df, \"dataset/test_standard_slash.txt\",token_tuple_to_str_slash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8c2ee21",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mdataset/test_standard.txt\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      5\u001b[39m     lines = f.readlines()\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m tok = \u001b[43mTupleTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_corpus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecial_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m[PAD]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m[UNK]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/python/fin_bert/train/train_tokenizer.py:42\u001b[39m, in \u001b[36mTupleTokenizer.from_corpus\u001b[39m\u001b[34m(cls, corpus_texts, min_freq, special_tokens)\u001b[39m\n\u001b[32m     40\u001b[39m         token2id[tok] = idx\n\u001b[32m     41\u001b[39m         idx += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m tokenizer = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken2id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspecial_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspecial_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/python/fin_bert/train/train_tokenizer.py:11\u001b[39m, in \u001b[36mTupleTokenizer.__init__\u001b[39m\u001b[34m(self, vocab, unk_token, pad_token, **kwargs)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vocab=\u001b[38;5;28;01mNone\u001b[39;00m, unk_token=\u001b[33m\"\u001b[39m\u001b[33m[UNK]\u001b[39m\u001b[33m\"\u001b[39m, pad_token=\u001b[33m\"\u001b[39m\u001b[33m[PAD]\u001b[39m\u001b[33m\"\u001b[39m, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# vocabulary: mapping from tuple-string to id\u001b[39;00m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mself\u001b[39m.vocab = vocab \u001b[38;5;129;01mor\u001b[39;00m {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hf/lib/python3.13/site-packages/transformers/tokenization_utils.py:438\u001b[39m, in \u001b[36mPreTrainedTokenizer.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(**kwargs)\n\u001b[32m    436\u001b[39m \u001b[38;5;66;03m# 4. If some of the special tokens are not part of the vocab, we add them, at the end.\u001b[39;00m\n\u001b[32m    437\u001b[39m \u001b[38;5;66;03m# the order of addition is the same as self.SPECIAL_TOKENS_ATTRIBUTES following `tokenizers`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_add_tokens\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mall_special_tokens_extended\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_added_tokens_encoder\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspecial_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[38;5;28mself\u001b[39m._decode_use_source_tokenizer = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hf/lib/python3.13/site-packages/transformers/tokenization_utils.py:546\u001b[39m, in \u001b[36mPreTrainedTokenizer._add_tokens\u001b[39m\u001b[34m(self, new_tokens, special_tokens)\u001b[39m\n\u001b[32m    544\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m added_tokens\n\u001b[32m    545\u001b[39m \u001b[38;5;66;03m# TODO this is fairly slow to improve!\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m546\u001b[39m current_vocab = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.copy()\n\u001b[32m    547\u001b[39m new_idx = \u001b[38;5;28mlen\u001b[39m(current_vocab)  \u001b[38;5;66;03m# only call this once, len gives the last index + 1\u001b[39;00m\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m new_tokens:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hf/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:1525\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.get_vocab\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_vocab\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m   1516\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[33;03m    Returns the vocabulary as a dictionary of token to index.\u001b[39;00m\n\u001b[32m   1518\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1523\u001b[39m \u001b[33;03m        `Dict[str, int]`: The vocabulary.\u001b[39;00m\n\u001b[32m   1524\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1525\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n",
      "\u001b[31mNotImplementedError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from train.train_tokenizer import TupleTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "with open(\"dataset/test_standard.txt\",\"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "tok = TupleTokenizer.from_corpus(lines, min_freq=1, special_tokens=[\"[PAD]\",\"[UNK]\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
